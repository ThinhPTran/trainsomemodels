{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "041e7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from numpy import expand_dims\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d4f8d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dense softmax model\n",
    "model = load_model('./Data/relu_softmax_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "65aab42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7e419fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6da93dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# make suare images have shape (28, 28, 1)\n",
    "x_train = np.reshape(x_train, (60000, 784))\n",
    "x_test = np.reshape(x_test, (10000, 784))\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f222fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18f4142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_val_img = x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "66938781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool\n",
    "model.predict(np.array([a_val_img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bcdc6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert code keras2caffe\n",
    "import caffe\n",
    "from caffe import layers as L, params as P\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def set_padding(config_keras, input_shape, config_caffe):\n",
    "    if config_keras['padding']=='valid':\n",
    "        return\n",
    "    elif config_keras['padding']=='same':\n",
    "        #pad = ((layer.output_shape[1] - 1)*strides[0] + pool_size[0] - layer.input_shape[1])/2\n",
    "        #pad=pool_size[0]/(strides[0]*2)\n",
    "        #pad = (pool_size[0]*layer.output_shape[1] - (pool_size[0]-strides[0])*(layer.output_shape[1]-1) - layer.input_shape[1])/2\n",
    "        \n",
    "        if 'kernel_size' in config_keras:\n",
    "            kernel_size = config_keras['kernel_size']\n",
    "        elif 'pool_size' in config_keras:\n",
    "            kernel_size = config_keras['pool_size']\n",
    "        else:\n",
    "            raise Exception('Undefined kernel size')\n",
    "        \n",
    "        #pad_w = int(kernel_size[1] // 2)\n",
    "        #pad_h = int(kernel_size[0] // 2)\n",
    "        \n",
    "        strides = config_keras['strides']\n",
    "        w = input_shape[1]\n",
    "        h = input_shape[2]\n",
    "        \n",
    "        out_w = math.ceil(w / float(strides[1]))\n",
    "        pad_w = int((kernel_size[1]*out_w - (kernel_size[1]-strides[1])*(out_w - 1) - w)/2)\n",
    "        \n",
    "        out_h = math.ceil(h / float(strides[0]))\n",
    "        pad_h = int((kernel_size[0]*out_h - (kernel_size[0]-strides[0])*(out_h - 1) - h)/2)\n",
    "        \n",
    "        if pad_w==0 and pad_h==0:\n",
    "            return\n",
    "        \n",
    "        if pad_w==pad_h:\n",
    "            config_caffe['pad'] = pad_w\n",
    "        else:\n",
    "            config_caffe['pad_h'] = pad_h\n",
    "            config_caffe['pad_w'] = pad_w\n",
    "        \n",
    "    else:\n",
    "        raise Exception(config_keras['padding']+' padding is not supported')\n",
    "\n",
    "def convert(keras_model, caffe_net_file, caffe_params_file):\n",
    "    \n",
    "    caffe_net = caffe.NetSpec()\n",
    "    \n",
    "    net_params = dict()\n",
    "    \n",
    "    outputs=dict()\n",
    "    shape=()\n",
    "    \n",
    "    input_str = ''\n",
    "    \n",
    "    # Thinh Port start\n",
    "    \n",
    "    caffe_net['data'] = L.Layer()\n",
    "    input_str = 'input: {}\\ninput_dim: {}\\ninput_dim: {}\\ninput_dim: {}\\ninput_dim: {}'.format('\"' + 'data' + '\"',\n",
    "                1, 60000, 28, 28)\n",
    "    outputs['input_3'] = 'data'\n",
    "    \n",
    "    print(\"caffe_net tops: \");\n",
    "    print(len(caffe_net.tops)); \n",
    "    \n",
    "    # Thinh port end\n",
    "            \n",
    "\n",
    "    \n",
    "    for layer in keras_model.layers:\n",
    "        \n",
    "        name = layer.name\n",
    "        layer_type = type(layer).__name__\n",
    "        \n",
    "        config = layer.get_config()\n",
    "        \n",
    "        print(\"name %s, layer_type: %s, input_name: %s\" %(name, layer_type, layer.input.name)); \n",
    "        print(\"Config!!!\")\n",
    "        print(config)\n",
    "        print(\"caffe_net tops: \");\n",
    "        print(len(caffe_net.tops)); \n",
    "\n",
    "        blobs = layer.get_weights()\n",
    "        blobs_num = len(blobs)\n",
    "        \n",
    "        if type(layer.output)==list:\n",
    "            raise Exception('Layers with multiply outputs are not supported')\n",
    "        else: \n",
    "            top=layer.output.name\n",
    "        \n",
    "        if type(layer.input)!=list:\n",
    "            bottom = layer.input.name\n",
    "        \n",
    "        #first we need to create Input layer\n",
    "        if layer_type=='InputLayer' or len(caffe_net.tops)==0:\n",
    "\n",
    "            input_name = 'data'\n",
    "            caffe_net[input_name] = L.Layer()\n",
    "            outputs[layer.input.name] = input_name\n",
    "            if layer_type=='InputLayer':\n",
    "                continue\n",
    "                \n",
    "        if layer_type=='Conv2D' or layer_type=='Convolution2D':\n",
    "            \n",
    "            strides = config['strides']\n",
    "            kernel_size = config['kernel_size']\n",
    "            \n",
    "            kwargs = { 'num_output': config['filters'] }\n",
    "            \n",
    "            if kernel_size[0]==kernel_size[1]:\n",
    "            \tkwargs['kernel_size']=kernel_size[0]\n",
    "            else:\n",
    "            \tkwargs['kernel_h']=kernel_size[0]\n",
    "            \tkwargs['kernel_w']=kernel_size[1]\n",
    "            \n",
    "            if strides[0]==strides[1]:\n",
    "            \tkwargs['stride']=strides[0]\n",
    "            else:\n",
    "            \tkwargs['stride_h']=strides[0]\n",
    "            \tkwargs['stride_w']=strides[1]\n",
    "            \n",
    "            if not config['use_bias']:\n",
    "            \tkwargs['bias_term'] = False\n",
    "            \t#kwargs['param']=[dict(lr_mult=0)]\n",
    "            else:\n",
    "                #kwargs['param']=[dict(lr_mult=0), dict(lr_mult=0)]\n",
    "                pass\n",
    "            \n",
    "            set_padding(config, layer.input_shape, kwargs)\n",
    "            \n",
    "            caffe_net[name] = L.Convolution(caffe_net[outputs[bottom]], **kwargs)\n",
    "            \n",
    "            blobs[0] = np.array(blobs[0]).transpose(3,2,0,1)\n",
    "            net_params[name] = blobs\n",
    "\n",
    "            if config['activation'] == 'relu':\n",
    "                name_s = name+'s'\n",
    "                caffe_net[name_s] = L.ReLU(caffe_net[name], in_place=True)\n",
    "            elif config['activation'] == 'sigmoid':\n",
    "                name_s = name+'s'\n",
    "                caffe_net[name_s] = L.Sigmoid(caffe_net[name], in_place=True)\n",
    "            elif config['activation'] == 'linear':\n",
    "                #do nothing\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception('Unsupported activation '+config['activation'])\n",
    "        \n",
    "        elif layer_type=='DepthwiseConv2D':\n",
    "            \n",
    "            strides = config['strides']\n",
    "            kernel_size = config['kernel_size']\n",
    "\n",
    "            kwargs = {'num_output': layer.input_shape[3]}\n",
    "\n",
    "            if kernel_size[0] == kernel_size[1]:\n",
    "                kwargs['kernel_size'] = kernel_size[0]\n",
    "            else:\n",
    "                kwargs['kernel_h'] = kernel_size[0]\n",
    "                kwargs['kernel_w'] = kernel_size[1]\n",
    "\n",
    "            if strides[0] == strides[1]:\n",
    "                kwargs['stride'] = strides[0]\n",
    "            else:\n",
    "                kwargs['stride_h'] = strides[0]\n",
    "                kwargs['stride_w'] = strides[1]\n",
    "\n",
    "            set_padding(config, layer.input_shape, kwargs)\n",
    "\n",
    "            kwargs['group'] = layer.input_shape[3]\n",
    "\n",
    "            kwargs['bias_term'] = False\n",
    "            caffe_net[name] = L.Convolution(caffe_net[outputs[bottom]], **kwargs)\n",
    "            blob = np.array(blobs[0]).transpose(2, 3, 0, 1)\n",
    "            blob.shape = (1,) + blob.shape\n",
    "            net_params[name] = blob\n",
    "            \n",
    "            if config['activation'] == 'relu':\n",
    "                name_s = name+'s'\n",
    "                caffe_net[name_s] = L.ReLU(caffe_net[name], in_place=True)\n",
    "            elif config['activation'] == 'sigmoid':\n",
    "                name_s = name+'s'\n",
    "                caffe_net[name_s] = L.Sigmoid(caffe_net[name], in_place=True)\n",
    "            elif config['activation'] == 'linear':\n",
    "                #do nothing\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception('Unsupported activation '+config['activation'])\n",
    "\n",
    "        elif layer_type == 'SeparableConv2D':\n",
    "\n",
    "            strides = config['strides']\n",
    "            kernel_size = config['kernel_size']\n",
    "\n",
    "            kwargs = {'num_output': layer.input_shape[3]}\n",
    "\n",
    "            if kernel_size[0] == kernel_size[1]:\n",
    "                kwargs['kernel_size'] = kernel_size[0]\n",
    "            else:\n",
    "                kwargs['kernel_h'] = kernel_size[0]\n",
    "                kwargs['kernel_w'] = kernel_size[1]\n",
    "\n",
    "            if strides[0] == strides[1]:\n",
    "                kwargs['stride'] = strides[0]\n",
    "            else:\n",
    "                kwargs['stride_h'] = strides[0]\n",
    "                kwargs['stride_w'] = strides[1]\n",
    "\n",
    "            set_padding(config, layer.input_shape, kwargs)\n",
    "\n",
    "            kwargs['group'] = layer.input_shape[3]\n",
    "\n",
    "            kwargs['bias_term'] = False\n",
    "            caffe_net[name] = L.Convolution(caffe_net[outputs[bottom]], **kwargs)\n",
    "            blob = np.array(blobs[0]).transpose(2, 3, 0, 1)\n",
    "            blob.shape = (1,) + blob.shape\n",
    "            net_params[name] = blob\n",
    "\n",
    "            name2 = name + '_'\n",
    "            kwargs = {'num_output': config['filters'], 'kernel_size': 1, 'bias_term': config['use_bias']}\n",
    "            caffe_net[name2] = L.Convolution(caffe_net[name], **kwargs)\n",
    "\n",
    "            if config['use_bias'] == True:\n",
    "                blob2 = []\n",
    "                blob2.append(np.array(blobs[1]).transpose(3, 2, 0, 1))\n",
    "                blob2.append(np.array(blobs[2]))\n",
    "                blob2[0].shape = (1,) + blob2[0].shape\n",
    "            else:\n",
    "                blob2 = np.array(blobs[1]).transpose(3, 2, 0, 1)\n",
    "                blob2.shape = (1,) + blob2.shape\n",
    "\n",
    "            net_params[name2] = blob2\n",
    "            name = name2\n",
    "\n",
    "        elif layer_type=='BatchNormalization':\n",
    "            \n",
    "            param = dict()\n",
    "            \n",
    "            variance = np.array(blobs[-1])\n",
    "            mean = np.array(blobs[-2])\n",
    "            \n",
    "            if config['scale']:\n",
    "                gamma = np.array(blobs[0])\n",
    "                sparam=[dict(lr_mult=1), dict(lr_mult=1)]\n",
    "            else:\n",
    "                gamma = np.ones(mean.shape, dtype=np.float32)\n",
    "                #sparam=[dict(lr_mult=0, decay_mult=0), dict(lr_mult=1, decay_mult=1)]\n",
    "                sparam=[dict(lr_mult=0), dict(lr_mult=1)]\n",
    "                #sparam=[dict(lr_mult=0), dict(lr_mult=0)]\n",
    "            \n",
    "            if config['center']:\n",
    "                beta = np.array(blobs[-3])\n",
    "                param['bias_term']=True\n",
    "            else:\n",
    "                beta = np.zeros(mean.shape, dtype=np.float32)\n",
    "                param['bias_term']=False\n",
    "            \n",
    "            caffe_net[name] = L.BatchNorm(caffe_net[outputs[bottom]], in_place=True)\n",
    "            \t#param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=1, decay_mult=1), dict(lr_mult=0, decay_mult=0)])\n",
    "            \t#param=[dict(lr_mult=1), dict(lr_mult=1), dict(lr_mult=0)])\n",
    "                \n",
    "            net_params[name] = (mean, variance, np.array(1.0)) \n",
    "            \n",
    "            name_s = name+'s'\n",
    "            \n",
    "            caffe_net[name_s] = L.Scale(caffe_net[name], in_place=True, \n",
    "            \tparam=sparam, scale_param={'bias_term': config['center']})\n",
    "            net_params[name_s] = (gamma, beta)\n",
    "            \n",
    "        elif layer_type=='Dense':\n",
    "            caffe_net[name] = L.InnerProduct(caffe_net[outputs[bottom]], \n",
    "            \tnum_output=config['units'], weight_filler=dict(type='xavier'))\n",
    "            \n",
    "            if config['use_bias']:\n",
    "                weight=np.array(blobs[0]).transpose(1, 0)\n",
    "                if type(layer._inbound_nodes[0].inbound_layers[0]).__name__=='Flatten':\n",
    "                    flatten_shape=layer._inbound_nodes[0].inbound_layers[0].input_shape\n",
    "                    for i in range(weight.shape[0]):\n",
    "                        weight[i]=np.array(weight[i].reshape(flatten_shape[1],flatten_shape[2],flatten_shape[3]).transpose(2,0,1).reshape(weight.shape[1]))\n",
    "                net_params[name] = (weight, np.array(blobs[1]))\n",
    "            else:\n",
    "                net_params[name] = (blobs[0])\n",
    "                \n",
    "            name_s = name+'s'\n",
    "            if config['activation']=='softmax':\n",
    "                caffe_net[name_s] = L.Softmax(caffe_net[name], in_place=True)\n",
    "            elif config['activation']=='relu':\n",
    "                caffe_net[name_s] = L.ReLU(caffe_net[name], in_place=True)\n",
    "        \n",
    "        elif layer_type=='Activation':\n",
    "            if config['activation']=='relu':\n",
    "                #caffe_net[name] = L.ReLU(caffe_net[outputs[bottom]], in_place=True)\n",
    "                if len(layer.input.consumers())>1:\n",
    "                    caffe_net[name] = L.ReLU(caffe_net[outputs[bottom]])\n",
    "                else:\n",
    "                    caffe_net[name] = L.ReLU(caffe_net[outputs[bottom]], in_place=True)\n",
    "            elif config['activation']=='relu6':\n",
    "                #TODO\n",
    "                caffe_net[name] = L.ReLU(caffe_net[outputs[bottom]])\n",
    "            elif config['activation']=='softmax':\n",
    "                caffe_net[name] = L.Softmax(caffe_net[outputs[bottom]], in_place=True)\n",
    "            elif config['activation'] == 'sigmoid':\n",
    "                # name_s = name+'s'\n",
    "                caffe_net[name] = L.Sigmoid(caffe_net[outputs[bottom]], in_place=True)\n",
    "            else:\n",
    "                raise Exception('Unsupported activation '+config['activation'])\n",
    "        \n",
    "        elif layer_type=='Cropping2D':\n",
    "            shape = layer.output_shape\n",
    "            ddata = L.DummyData(shape=dict(dim=[1, shape[3],shape[1], shape[2]]))\n",
    "            layers = []\n",
    "            layers.append(caffe_net[outputs[bottom]])   \n",
    "            layers.append(ddata)   #TODO\n",
    "            caffe_net[name] = L.Crop(*layers)\n",
    "        \n",
    "        elif layer_type=='Concatenate' or layer_type=='Merge':\n",
    "            layers = []\n",
    "            for i in layer.input:\n",
    "                layers.append(caffe_net[outputs[i.name]])\n",
    "            caffe_net[name] = L.Concat(*layers, axis=1)\n",
    "        \n",
    "        elif layer_type=='Add':\n",
    "            layers = []\n",
    "            for i in layer.input:\n",
    "                layers.append(caffe_net[outputs[i.name]])\n",
    "            caffe_net[name] = L.Eltwise(*layers)\n",
    "        \n",
    "        elif layer_type=='Flatten':\n",
    "            print(\"Flatten\\n\")\n",
    "            print(bottom)\n",
    "            print(outputs)\n",
    "            caffe_net[name] = L.Flatten(caffe_net[outputs[bottom]])\n",
    "        \n",
    "        elif layer_type=='Reshape':\n",
    "            shape = config['target_shape']\n",
    "            if len(shape)==3:\n",
    "                #shape = (layer.input_shape[0], shape[2], shape[0], shape[1])\n",
    "                shape = (1, shape[2], shape[0], shape[1])\n",
    "            elif len(shape)==1:\n",
    "                #shape = (layer.input_shape[0], 1, 1, shape[0])\n",
    "                shape = (1, 1, 1, shape[0])\n",
    "            caffe_net[name] = L.Reshape(caffe_net[outputs[bottom]], \n",
    "                reshape_param={'shape':{'dim': list(shape)}})\n",
    "        \n",
    "        elif layer_type=='MaxPooling2D' or layer_type=='AveragePooling2D':\n",
    "            \n",
    "            kwargs={}\n",
    "            \n",
    "            if layer_type=='MaxPooling2D':\n",
    "                kwargs['pool'] = P.Pooling.MAX\n",
    "            else:\n",
    "                kwargs['pool'] = P.Pooling.AVE\n",
    "                \n",
    "            pool_size = config['pool_size']\n",
    "            strides  = config['strides']\n",
    "            \n",
    "            if pool_size[0]!=pool_size[1]:\n",
    "                raise Exception('Unsupported pool_size')\n",
    "                    \n",
    "            if strides[0]!=strides[1]:\n",
    "                raise Exception('Unsupported strides')\n",
    "            \n",
    "            set_padding(config, layer.input_shape, kwargs)\n",
    "            \n",
    "            caffe_net[name] = L.Pooling(caffe_net[outputs[bottom]], kernel_size=pool_size[0], \n",
    "                stride=strides[0], **kwargs)\n",
    "        \n",
    "        elif layer_type=='Dropout':\n",
    "            caffe_net[name] = L.Dropout(caffe_net[outputs[bottom]], \n",
    "                dropout_param=dict(dropout_ratio=config['rate']))\n",
    "        \n",
    "        elif layer_type=='GlobalAveragePooling2D':\n",
    "            caffe_net[name] = L.Pooling(caffe_net[outputs[bottom]], pool=P.Pooling.AVE, \n",
    "                pooling_param=dict(global_pooling=True))\n",
    "        \n",
    "        elif layer_type=='UpSampling2D':\n",
    "            if config['size'][0]!=config['size'][1]:\n",
    "                raise Exception('Unsupported upsampling factor')\n",
    "            factor = config['size'][0]\n",
    "            kernel_size = 2 * factor - factor % 2\n",
    "            stride = factor\n",
    "            pad = int(math.ceil((factor - 1) / 2.0))\n",
    "            channels = layer.input_shape[-1]\n",
    "            caffe_net[name] = L.Deconvolution(caffe_net[outputs[bottom]], convolution_param=dict(num_output=channels, \n",
    "                group=channels, kernel_size=kernel_size, stride=stride, pad=pad, weight_filler=dict(type='bilinear'), \n",
    "                bias_term=False), param=dict(lr_mult=0, decay_mult=0))\n",
    "        \n",
    "        elif layer_type=='LeakyReLU':\n",
    "            caffe_net[name] = L.ReLU(caffe_net[outputs[bottom]], negative_slope=config['alpha'], in_place=True)\n",
    "\n",
    "        #TODO\n",
    "        elif layer_type=='ZeroPadding2D':\n",
    "            padding=config['padding']\n",
    "            #ch = layer.input_shape[3]\n",
    "            #caffe_net[name] = L.Convolution(caffe_net[outputs[bottom]], num_output=ch, kernel_size=1, stride=1, group=ch,\n",
    "            #    pad_h=padding[0][0], pad_w=padding[1][0], convolution_param=dict(bias_term = False))\n",
    "            #params = np.ones((1,ch,1,1))\n",
    "            \n",
    "            #net_params[name] = np.ones((1,ch,1,1,1))\n",
    "            #net_params[name] = np.ones(layer.output_shape)\n",
    "            \n",
    "            caffe_net[name] = L.Pooling(caffe_net[outputs[bottom]], kernel_size=1, \n",
    "                stride=1, pad_h=padding[0][0]+padding[0][1], pad_w=padding[1][0]+padding[1][1], pool=P.Pooling.AVE)\n",
    "        \n",
    "        else:\n",
    "            raise Exception('Unsupported layer type: '+layer_type)\n",
    "            \n",
    "        outputs[top]=name\n",
    "        \n",
    "    \n",
    "    #replace empty layer with input blob\n",
    "    net_proto = input_str + '\\n' + 'layer {' + 'layer {'.join(str(caffe_net.to_proto()).split('layer {')[2:])\n",
    "    \n",
    "    f = open(caffe_net_file, 'w') \n",
    "    f.write(net_proto)\n",
    "    f.close()\n",
    "    \n",
    "    caffe_model = caffe.Net(caffe_net_file, caffe.TEST)\n",
    "    \n",
    "    for layer in caffe_model.params.keys():\n",
    "        if 'up_sampling2d' in layer:\n",
    "            continue\n",
    "        for n in range(0, len(caffe_model.params[layer])):\n",
    "            caffe_model.params[layer][n].data[...] = net_params[layer][n]\n",
    "\n",
    "    caffe_model.save(caffe_params_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eac6ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "caffe_proto = 'dense_softmax_model.prototxt'\n",
    "caffe_weights = 'dense_softmax_model.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2e05997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caffe_net tops: \n",
      "1\n",
      "name dense_6, layer_type: Dense, input_name: input_3\n",
      "Config!!!\n",
      "{'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "caffe_net tops: \n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'InputLayer' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [147]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaffe_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaffe_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [145]\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(keras_model, caffe_net_file, caffe_params_file)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_bias\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    277\u001b[0m     weight\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(blobs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inbound_nodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minbound_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlatten\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    279\u001b[0m         flatten_shape\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39m_inbound_nodes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minbound_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_shape\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'InputLayer' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "convert(model, caffe_proto, caffe_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba565ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
